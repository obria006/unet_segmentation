{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export PyTorch Model to ONNX\n",
    "\n",
    "Zahra needs `.onnx` files to be used in Labview, but PyTorch saves the model weights/checkpoints as `.pth` files. Hence, we need to conver the PyTorch `.pth` weights to `.onnx` weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that converts model to `.onnx`. As arguments, it takes the trained model, a `torch.Tensor` of the the same size *(BATCH, CHANNEL, HEIGHT, WIDTH)* as is expected by the model (the values in the tensor aren't important), and the filepath of where to save the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff for making Ryan's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tifffile as tiff\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn\n",
    "\n",
    "sns.set_theme(style = \"white\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "def FinalBlock(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        #ConvBlock(in_channels, in_channels,  kernel_size = 1),\n",
    "        ConvBlock(in_channels, in_channels,  kernel_size = 1, stride = 1, padding = 0),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1, padding = 0)\n",
    "    )\n",
    "\n",
    "def MiddleBlock(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        ConvBlock(in_channels, out_channels),\n",
    "        nn.Dropout(p = 0.2),\n",
    "        ConvBlock(out_channels, out_channels)\n",
    "    )\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\" Residual encoder block. \"\"\"\n",
    "    def __init__(self, in_channels, feature_maps, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = (2, 2), stride = None)\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, feature_maps, kernel_size = (1, 1), stride = stride, bias = False),\n",
    "            nn.BatchNorm2d(feature_maps)\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, feature_maps,  kernel_size = (3, 3), stride = stride, padding = 1, bias = False)\n",
    "        self.bn1   = nn.BatchNorm2d(feature_maps)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_maps, feature_maps, kernel_size = (3, 3), stride = 1,      padding = 1, bias = False)\n",
    "        self.bn2   = nn.BatchNorm2d(feature_maps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        skip_connection = self.relu(x)\n",
    "\n",
    "        x = self.maxpool(skip_connection)\n",
    "\n",
    "        return x, skip_connection\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.Encoder_0 = ResidualBlock(in_channels = in_channels, feature_maps = 32)\n",
    "        self.Encoder_1 = ResidualBlock(in_channels = 32,          feature_maps = 64)\n",
    "        self.Encoder_2 = ResidualBlock(in_channels = 64,          feature_maps = 128)\n",
    "\n",
    "        self.Middle = MiddleBlock(in_channels = 128, out_channels = 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x0 = self.Encoder_0(x)\n",
    "        x, x1 = self.Encoder_1(x)\n",
    "        x, x2 = self.Encoder_2(x)\n",
    "\n",
    "        x3 = self.Middle(x)\n",
    "\n",
    "        return [x0, x1, x2, x3]\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "        self.conv_block_0 = ConvBlock(in_channels + out_channels, out_channels)\n",
    "        self.conv_block_1 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = torch.cat((x, skip_connection), 1)\n",
    "\n",
    "        x = self.conv_block_0(x)\n",
    "        x = self.conv_block_1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder_0 = DecoderBlock(256, 128) \n",
    "        self.decoder_1 = DecoderBlock(128, 64)\n",
    "        self.decoder_2 = DecoderBlock(64, 32)\n",
    "        \n",
    "        self.FinalBlock = FinalBlock(in_channels = 32, out_channels = out_channels)\n",
    "        \n",
    "    def forward(self, x0, x1, x2, x3):\n",
    "        x = self.decoder_0(x3, x2)\n",
    "        x = self.decoder_1(x,  x1)\n",
    "        x = self.decoder_2(x,  x0)\n",
    "        \n",
    "        x = self.FinalBlock(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels = 1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.backbone = Encoder(in_channels)\n",
    "        self.head     = Decoder(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0, x1, x2, x3 = self.backbone(x)\n",
    "\n",
    "        x = self.head(x0, x1, x2, x3)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 512, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the preprocessing I do to this image.\n",
    "def preprocess_image(image, max_value = 65_533):\n",
    "    \"\"\" Normalize values between -1 and 1. \"\"\"\n",
    "    return ((image / max_value) - 1) * 2\n",
    "\n",
    "model = UNet(1, 1)\n",
    "model.load_state_dict(torch.load(\"./model_0_cpu.pth\")[\"model_state_dict\"])\n",
    "\n",
    "# Verify output size is as expected.\n",
    "model(torch.rand(2, 1, 512, 512)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for making ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.onnx\n",
    "import onnx\n",
    "\n",
    "def my_export_onnx(model:torch.nn.Module, im:torch.Tensor, filepath:str, cpu:bool = True):\n",
    "    \"\"\"\n",
    "    Export model to `.onnx` file. \n",
    "    \n",
    "    Args:\n",
    "        model: Model to convert to .onnx file\n",
    "        im (torch.Tensor): Input tensor of expected size for inference\n",
    "        filepath (str): Location to save file\n",
    "        cpu (bool): True to send to cpu before export\n",
    "    \"\"\"\n",
    "    save_dir = Path(filepath).parent.absolute()\n",
    "    if not os.path.isdir(save_dir):\n",
    "        raise ValueError(f\"Invalid path to save: {filepath}. Parent directory doesn't exist\")\n",
    "    exten = Path(filepath).suffix\n",
    "    if exten != \".onnx\":\n",
    "        raise ValueError(f\"Invalid path to save: {filepath}. Must be `.onnx` file.\")\n",
    "    _shape = im.shape\n",
    "    if len(_shape) != 4:\n",
    "        raise ValueError(f\"Invalid input tensor shape {_shape}. Must be (?, 1, ?, ?) -> (B, C, H, W).\")\n",
    "    if _shape[1] != 1:\n",
    "        raise ValueError(f\"Invalid input tensor shape {_shape}. Must have 1 channel -> (B, C, H, W).\")\n",
    "    \n",
    "\n",
    "    print(f\"Starting `.onnx.` export to: {filepath}\")\n",
    "    \n",
    "    # set the model to inference mode \n",
    "    model.eval()\n",
    "\n",
    "    if not cpu:\n",
    "        raise NotImplementedError(\"Must use cpu\")\n",
    "    else:\n",
    "        model = model.cpu()\n",
    "        im = im.cpu()\n",
    "\n",
    "    # Export model to .onnx file\n",
    "    torch.onnx.export(\n",
    "        model,                          # Model to save\n",
    "        im,                             # Dummy torch.Tensor of expected size\n",
    "        filepath,                       # Filepath to save\n",
    "        export_params = True,           # store the trained parameter weights inside the model file \n",
    "        opset_version = 12,             # the ONNX version to export the model to\n",
    "        do_constant_folding = True,     # whether to execute constant folding for optimization\n",
    "        input_names = ['images'],       # the model's input names\n",
    "        output_names = ['outputs'],     # the model's output names\n",
    "        dynamic_axes = {                # Axes of inputs outputs that can change at runtime (aka diff batch size than im )\n",
    "            \"images\": {0: \"batch_size\"},\n",
    "            \"outputs\": {0: \"batch_size\"},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Checks\n",
    "    model_onnx = onnx.load(filepath)  # load onnx model\n",
    "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "\n",
    "    print(\"ONNX file successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model input of size torch.Size([1, 1, 512, 512]) (B, C, H, W)\n"
     ]
    }
   ],
   "source": [
    "# Create \"dummy\" input tensor of expected size/shape for the model\n",
    "batch = 1\n",
    "channel = 1\n",
    "height = 512\n",
    "width = 512\n",
    "im = torch.zeros(batch, channel, height, width)\n",
    "print(f\"Created model input of size {im.shape} (B, C, H, W)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n",
      "torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "out = model(im)\n",
    "print(im.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting `.onnx.` export to: model_0_cpu.onnx\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX file successfully created!\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "onnx_name = \"model_0_cpu.onnx\"\n",
    "my_export_onnx(model=model, im=im, filepath=onnx_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCT_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
